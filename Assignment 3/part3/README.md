
As we need to implement HMM, I started with calculating transition, emission and initial probabilities for each word in the training set.
I had created a dict (word_freq) with each word as key  and value of that key is a another dict with keys as {'count':1,'initial_count':0,'ADJ':0,'ADV':0,'ADP':0,'CONJ':0,'DET':0,'NOUN':0,'NUM':0,'PRON':0,'PRT':0,'VERB':0,'X':0,'.':0}.
I was using the count key for calculating prior probability , initial_count key to calculate initial probability of that particular word and other keys to calculate emission probabilities for each label for each word. 
For transition probability i had made as 12*12 dict of dict (you can see it in the code). here i was calculating probability of NOUN->NOUN, NOUN->ADJ, NOUN->CONJ basically every possible transition of pos. initially i thaught it wil be needed for Viterbi. But, spending so much time on this i realised this will not help :). I have kept my code for this part in the file. Please check calculate_probabilities function to check it out.

After that, i started again, this time i wanted to implement bayes net first and then move to viterbi. I completed the implementation for bayes net but accuracy was not that great.For test_image-0_0 i was getting below output -->Simple: SUPREME CQURT?QEjTHE UNITEDpSTATES. As you can see accuracy was not that great. I experimented a lot to increase the accurecy like taking into account black_non_matched pixels, whites pixels but it didn't increase the accuracy when the image had a lot of noise. So, after a lot of experimentation with assigning weights for emmision probability i observerd that when the input image has a lot of noise then  assigning more weight for black_matched pixels gives accurate results and when the pixel density is less then assigning slightly less weight to back_match pixels gives accurate results. So I have incorporated this logic for getting different emission prob based on the density of the image. 

For viterbi I started with calculating emission probabilities and initial probabilities based on the training data. Viterbi seemed very hard and i was not able to do it. I watched the module for viterbi once again but still was not able to clearly think how to implement it. One of my teammate told me about the viterbi code provided by Prof. David in one of the in class activity. So, i looked at the code and implemented similar logic for my problem and it worked !! (I was so happy ;) ). I got some issues like probabilities mutplications were giving 0 ans, string was not correct etc but i was able to fix everything and now getting a decent output.

First letter of the string is getting printed wrong somehow. I looked at my logic for calculating initial_probabilities and made few changes but still not able to get correct ans for first letter. I don't know what to do for that part

For all the test images provided (0-19) i am able to get accuracy of over 90% (I think) and I'm happy with it.
